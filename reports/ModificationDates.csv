MIT License

Copyright (c) 2023 jbrandt130

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
# alexweb

Scripts for tracking changes to Ukrainian records on https://uk.wikisource.org.
{
    "DAARC":    "Архів:ДААРК",
    "DACHGO":   "Архів:ДАЧгО",
    "DACHkO":   "Архів:ДАЧкО",
    "DACHvO":   null,
    "DADNO":    "Архів:ДАДнО",
    "DADoO":    null,
    "DAHO":     "Архів:ДАХО",
    "DAHeO":    "Архів:ДАХеО",
    "DAHmO":    "Архів:ДАХмО",
    "DAIFO":    "Архів:ДАІФО",
    "DAK":      "Архів:ДАК",
    "DAKO":     "Архів:ДАКО",
    "DAKrO":    "Архів:ДАКрО",
    "DALO":     "Архів:ДАЛО",
    "DALuO":    "Архів:ДАЛуО",
    "DAMO":     "Архів:ДАМО",
    "DAOO":     "Архів:ДАОО",
    "DAPO":     "Архів:ДАПО",
    "DARO":     "Архів:ДАРО",
    "DAS":      "Архів:ДАС",
    "DASO":     "Архів:ДАСО",
    "DATE":     null,
    "DAVIO":    "Архів:ДАВіО",
    "DAVOO":    "Архів:ДАВоО",
    "DAZHO":    "Архів:ДАЖО",
    "DAZKO":    "Архів:ДАЗкО",
    "DAZpO":    "Архів:ДАЗпО"
}

#
# Ukraine records archive scraper

import pandas as pd
import numpy as np
import urllib.request, urllib.parse, urllib.error
import io
import re
import json
import datetime
import os
import random
from time import sleep



archive_base = base='https://uk.wikisource.org'
column_names = [ '№' , 'Опис', 'Номер', 'Фонд' ]
subarchives = ['Д', 'Р', 'П']

with open('archives.json') as f:
    archive_list = json.load(f)

with open('months.json') as f:
    uk_months = json.load(f)


def pick_table(tables, column_index=0):
    if tables is None:
        return None
    for table in tables:
        try:
            if table.columns[0][column_index] in column_names:
                return table
        except:
            pass
    print('pick_table: not found')
    return None

def extract_table(df, only_linked=True):
    if df is None:
        return []
    table = df.to_numpy()
    table = table.tolist()
    result = [row[0] for row in table]
    #print (result)
    result = map(lambda x: (x[0], None) if x[1] is None or 'redlink' in x[1] else x, result)
    if only_linked:
        result = filter(lambda x: x[1] is not None, result)
    return list(result)

def web_throttle(scale_factor = 1):
    web_delay_limit = 1 # seconds
    sleep((1. + random.random() * web_delay_limit) * scale_factor)

def open_url(url, request_timeout=5):
    print('open_url:', url)
    tries = 1
    try_limit = 3
    while tries <= try_limit:
        web_throttle(tries)
        result = None
        try:
            result = urllib.request.urlopen(url, timeout=request_timeout)
        except urllib.error.HTTPError as e:
            if e.code == 404:
                print('404: Page not found')
                return None
            else:
                print('HTTP error:', e.code, e.reason)
        except urllib.error.URLError as e:
            print('URL error:', e.reason)
        except BaseException as e:
            print('exception in urlopen:', type(e))
        if result is not None:
            return result
        print(f'FAILED open_url(tries={tries}): {url}')
        tries += 1
    return None

def format_date(message, uk_months=uk_months):
    message = message.replace(',', '')
    message = message.split(' ')
    message = map(lambda x: uk_months[x] if x in uk_months else x, message)
    message = ','.join(reversed(list(message)))
    return message

lastmod_pattern = re.compile('[0-9][0-9]:[0-9][0-9].+[0-9][0-9]?.+[0-9][0-9][0-9][0-9]')

def lastmod(message):
    #print(message)
    if b'lastmod' in message:
        message = message.decode('utf-8')
        result = re.search(lastmod_pattern, message)
        if result is not None:
            return format_date(result.group(0))
        else:
            return message
    return None

def read_html(url):
    if url is None:
        return (None, None)
    file = open_url(url)
    if file is None:
        return (None, None)
    message = file.read()
    mod_date = lastmod(message)
    #print('read_html: Date:', mod_date)
    tables = None
    try:
        tables = pd.read_html(message, extract_links="all")
    except ImportError as e:
        print('ImportError encountered in read_html. Skipping...')
        pass
    return (tables, mod_date)

def get_lastmod(url):
    if url is not None:
        file = open_url(url)
        if file is None:
            return 'CONNECTION FAILED'
        return lastmod(file.read())
    return None

def scan_archive(archive, stream=None):
    output = Logger(stream)
    output.write(archive.report)
    for f in range(len(archive.children)):
        fond = Fond(archive.children[f], archive)
        output.write(fond.report)
        for o in range(len(fond.children)):
            opus = Opus(fond.children[o], fond)
            output.write(opus.report)
            for c in range(len(opus.children)):
                case = Case(opus.children[c], opus)
                output.write(case.report)

def run_report(items = archive_list):
    out_dir = './var/' + str(datetime.datetime.now())
    try:
        os.mkdir('var')
    except:
        pass
    os.mkdir(out_dir)
    print('reporting to', out_dir)
    for item in items:
        if items[item] is not None:
            with open(f'{out_dir}/{item}.csv', 'w') as file:
                for sub in subarchives:
                    print(f'scanning {item}/{sub}...')
                    try:
                        scan_archive(Archive(item, subarchive=sub), file)
                    except KeyboardInterrupt:
                        return
                    except BaseException as e:
                        print(f'... EXCEPTION occured while scanning {item}/{sub}')
                        print(e.with_traceback())

class Logger:
    def __init__(self, output = None):
        self._stream = output

    def write(self, message):
        if self._stream is None:
            print(message)
        else:
            self._stream.write(message + '\n')

class Table:
    def __init__(self, spec, parent, is_leaf=False):
        self._parent = parent
        self._spec = spec
        if is_leaf:
            self._lastmod = get_lastmod(self.url)
            self._table = None
            self._children = None
        else:
            result = read_html(self.url)
            self._lastmod = result[1]
            self._table = pick_table(result[0])
            if self._table is None:
                print('Table not found:', self.url)
            self._children = extract_table(self._table)

    @property
    def children(self):
        return self._children

    @property
    def base(self):
        return self._parent.base

    @property
    def url(self):
        return self.base + self._spec[1] if self._spec is not None else None

    @property
    def id(self):
        return self._spec[0]

    @property
    def name(self):
        return f'{self._parent.name}/{self.id}'

    @property
    def lastmod(self):
        return self._lastmod

    @property
    def child_class(self):
        return None

    @property
    def report(self):
        # make sure no commas in the name
        return f'{self.kind},{self.name.replace(',','')},{self.lastmod}'

    def lookup(self, entry_id):
        matches = [x for x in self._children if x[0] == entry_id]
        return self.child_class(matches[0], self) if len(matches) > 0 else None

class Archive(Table):
    def __init__(self, tag, archives=archive_list, subarchive=subarchives[0], base=archive_base):
        self._tag = tag
        archive_name = archives[tag] if tag in archive_list else None
        archive_name = f'{archive_name}/{subarchive}'
        self._name = archive_name
        self._subarchive = subarchive
        self._base = base
        super().__init__(None, None)

    @property
    def tag(self):
        return self._tag
    
    @property
    def name(self):
        return self._name

    @property
    def subarchive(self):
        return self._subarchive

    @property
    def base(self):
        return self._base
    
    @property
    def url(self):
        return self._base + '/wiki/' + str(urllib.parse.quote(self._name))

    @property
    def report(self):
        return f'{self.kind},{self.tag}/{self.subarchive},{self.lastmod}'

    @property
    def child_class(self):
        return Fond

    @property
    def kind(self):
        return 'archive'

class Fond(Table):
    @property
    def name(self):
        return f'{self._parent.tag}/{self.id}'

    @property
    def child_class(self):
        return Opus

    @property
    def kind(self):
        return 'fond'


class Opus(Table):
    @property
    def kind(self):
        return 'opus'

    @property
    def child_class(self):
        return Case


class Case(Table):
    def __init__(self, spec, opus):
        super().__init__(spec, opus, is_leaf=True)

    @property
    def kind(self):
        return 'case'


if __name__ == "__main__":
    print("Running archive report")
    run_report()

import pandas as pd
#import numpy as np
#import urllib.request, urllib.parse, urllib.error
import io
import re
import json
import datetime
import os
import random
#from time import sleep
import argparse 

data_dir = './var'

report_dir_pattern = re.compile('[0-9]+-[0-9]+-[0-9]+ [0-9]+:[0-9]+:[0-9]+.[0-9]+')

with open('archives.json') as f:
    archive_list = json.load(f)

def all_reports():
    dirs = os.listdir(data_dir)
    return list(sorted(filter(lambda x: re.match(report_dir_pattern, x) is not None, dirs), reverse=True))

def find_report(key, reports):
    if type(key) == int:
        if key < 0 or key >= len(reports):
            raise IndexError
        return key
    elif type(key) == str:
        if re.match('[0-9]+$', key) is not None:
            return find_report(int(key), reports)
        key = re.compile(key)
        for i, item in enumerate(reports):
            if re.search(key, item) is not None:
                return i
    raise ValueError

def form_date(df):
    df['date'] = df['year'] + ':' + df['month'] + ':' + df['day'] + ':' + df['hour']
    return df

def load_report(fname):
    #print('load_report', fname)
    df = pd.read_csv(f'{data_dir}/{fname}', 
                     names=['type', 'id', 'year', 'month', 'day', 'hour'],
                     dtype=str)
    df = form_date(df)
    df = df.drop(columns=['year', 'month', 'day', 'hour'])
    df = df.replace({0: ''})
    df = df.fillna('?')
    return df.set_index(['id'])

def check_changes(df1, df2):
    df = df1.join(df2, how='outer', lsuffix='1', rsuffix='2').fillna('')
    return df[df['date1'] != df['date2']]

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Alexweb Wiki Change Report')
    parser.add_argument('-t', '--target', help='target report for comparison (default: latest)', default=0)
    parser.add_argument('-r', '--reference', help='reference report for comparison (default: previous)', default=1)
    parser.add_argument('-l', '--list', action='store_true', help='list all available reports')
    args = parser.parse_args()

    #print("Running archive change report")
    reports = all_reports()

    if args.list:
        for i, report in enumerate(reports):
            print(f'[{i}] {report}')
    else:
        reference = find_report(args.reference, reports)
        target = find_report(args.target, reports)

        print('Change report:')
        print(f'    Target:    [{target}] {reports[target]}')
        print(f'    Reference: [{reference}] {reports[reference]}')

        reference = reports[reference]
        target = reports[target]

        for archive in archive_list:
            #print(f'Archive: {archive}')
            target_report = None
            reference_report = None
            try:
                target_report = load_report(f'{target}/{archive}.csv')
            except FileNotFoundError as e:
                print(f'No target report for {archive}.')
                pass
            try:
                reference_report = load_report(f'{reference}/{archive}.csv')
            except FileNotFoundError as e:
                print(f'No reference report for {archive}.')
                pass
            if target_report is not None and reference_report is not None:
                change = check_changes(reference_report, target_report)
                if len(change) > 0:
                    added = change[change['date1'] == '']
                    removed = change[change['date2'] == '']
                    updated = change[(change['date1'] != '') & (change['date2'] != '')]

                    for index, row in removed.iterrows():
                        print(f"MISSING {index} ({row['type1']}): {row['date1']}")
                    for index, row in added.iterrows():
                        print(f"ADDED {index} ({row['type2']}): {row['date2']}")
                    for index, row in updated.iterrows():
                        print(f"UPDATED {index} ({row['type2']}): {row['date1']} --> {row['date2']}")
            elif reference_report is not None:
                    for index, row in reference_report.iterrows():
                        print(f"MISSING {index} ({row['type']}): {row['date']}")
            elif target_report is not None:
                    for index, row in target_report.iterrows():
                        print(f"ADDED {index} ({row['type']}): {row['date']}")


#!/bin/bash -f

latest=`ls -t var | grep -v log | head -1`
echo Updating report using var/$latest
( cd "var/$latest"; wc -l `ls` ) > reports/RecordCounts.txt
( cd "var/$latest"; cat `ls` ) > reports/ModificationDates.csv
( cd reports ; wc -l `ls` )
echo Done
{
    "січня":        "01",
    "лютого":       "02",
    "березня":      "03",
    "квітня":       "04",
    "травня":       "05",
    "червня":       "06",
    "липня":        "07",
    "серпня":       "08",
    "вересня":      "09",
    "жовтня":       "10",
    "листопада":    "11",
    "грудня":       "12"
}pandas
numpy
bs4

#!/bin/bash

cd $HOME/root/alexweb
log=./var/log
python=/usr/local/anaconda3/bin/python
mkdir -p var
touch $log
echo starting report update: `date` >> $log
echo shell is $SHELL >> $log
#source $HOME/.zshrc
env >> $log
$python archivescraper.py >> $log 2>&1
./gen_report.sh >> $log 2>&1

change=`git status -s reports | wc -l`
if [ $change -gt 0 ]; then
    echo something changed >> $log
    date=`date` 
    git commit -a -m "automated report update: $date"
    git push
else
    echo no change >> $log
fi
echo finished report update: `date` >> $log

